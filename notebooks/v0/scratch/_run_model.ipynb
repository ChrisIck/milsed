{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/js7561/dev/milsed/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/js7561/dev/milsed/models\n"
     ]
    }
   ],
   "source": [
    "cd ../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/js7561/dev/milsed/models'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;34m00_setup.py\u001b[0m*    \u001b[38;5;34m02_train.py\u001b[0m*  \u001b[38;5;27m__pycache__\u001b[0m/\r\n",
      "\u001b[38;5;34m01_prepare.py\u001b[0m*  __init__.py   \u001b[38;5;27mresources\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python 02_train.py /beegfs/js7561/datasets/dcase2017/task4_official/train/features_silence/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import six\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import keras as K\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "import pescador\n",
    "import librosa\n",
    "import milsed.utils\n",
    "from jams.util import smkdirs\n",
    "\n",
    "OUTPUT_PATH = '/home/js7561/dev/milsed/models/resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pescador.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sampler(max_samples, duration, pump, seed):\n",
    "\n",
    "    n_frames = librosa.time_to_frames(duration,\n",
    "                                      sr=pump['mel'].sr,\n",
    "                                      hop_length=pump['mel'].hop_length)[0]\n",
    "\n",
    "    return pump.sampler(max_samples, n_frames-1, random_state=seed)\n",
    "\n",
    "\n",
    "def data_sampler(fname, sampler):\n",
    "    '''Generate samples from a specified h5 file'''\n",
    "    for datum in sampler(milsed.utils.load_h5(fname)):\n",
    "        yield datum\n",
    "\n",
    "\n",
    "def data_generator(working, tracks, sampler, k, augment=True, batch_size=32,\n",
    "                   **kwargs):\n",
    "    '''Generate a data stream from a collection of tracks and a sampler'''\n",
    "\n",
    "    seeds = []\n",
    "\n",
    "    for track in tracks:\n",
    "        fname = os.path.join(working,\n",
    "                             os.path.extsep.join([str(track), 'h5']))\n",
    "        seeds.append(pescador.Streamer(data_sampler, fname, sampler))\n",
    "\n",
    "        if augment:\n",
    "            for fname in sorted(glob(os.path.join(working,\n",
    "                                                  '{}.*.h5'.format(track)))):\n",
    "                seeds.append(pescador.Streamer(data_sampler, fname, sampler))\n",
    "\n",
    "    # Send it all to a mux\n",
    "    mux = pescador.Mux(seeds, k, **kwargs)\n",
    "\n",
    "    if batch_size == 1:\n",
    "        return mux\n",
    "    else:\n",
    "        return pescador.BufferedStreamer(mux, batch_size)\n",
    "\n",
    "\n",
    "def keras_tuples(gen, inputs=None, outputs=None):\n",
    "\n",
    "    if isinstance(inputs, six.string_types):\n",
    "        if isinstance(outputs, six.string_types):\n",
    "            # One input, one output\n",
    "            for datum in gen:\n",
    "                yield (datum[inputs], datum[outputs])\n",
    "        else:\n",
    "            # One input, multi outputs\n",
    "            for datum in gen:\n",
    "                yield (datum[inputs], [datum[o] for o in outputs])\n",
    "    else:\n",
    "        if isinstance(outputs, six.string_types):\n",
    "            for datum in gen:\n",
    "                yield ([datum[i] for i in inputs], datum[outputs])\n",
    "        else:\n",
    "            # One input, multi outputs\n",
    "            for datum in gen:\n",
    "                yield ([datum[i] for i in inputs],\n",
    "                       [datum[o] for o in outputs])\n",
    "\n",
    "\n",
    "def construct_model(pump, alpha):\n",
    "\n",
    "    model_inputs = ['mel/mag']\n",
    "\n",
    "    # Build the input layer\n",
    "    layers = pump.layers()\n",
    "\n",
    "    x_mag = layers['mel/mag']\n",
    "\n",
    "    # Apply batch normalization\n",
    "    x_bn = K.layers.BatchNormalization()(x_mag)\n",
    "\n",
    "    x_sq = milsed.layers.SqueezeLayer()(x_bn)\n",
    "\n",
    "    # First convolutional filter: a single 3-frame filters\n",
    "    conv1 = K.layers.Convolution1D(64, 3,\n",
    "                                   padding='same',\n",
    "                                   activation='relu',\n",
    "                                   kernel_initializer='he_uniform')(x_sq)\n",
    "                                   # data_format='channels_last')(x_sq)\n",
    "\n",
    "    # First recurrent layer: a 128-dim bidirectional gru\n",
    "    rnn1 = K.layers.Bidirectional(K.layers.GRU(128,\n",
    "                                               return_sequences=True))(conv1)\n",
    "\n",
    "    n_classes = pump.fields['static/tags'].shape[0]\n",
    "\n",
    "    p0 = K.layers.Dense(n_classes, activation='sigmoid')\n",
    "\n",
    "    p_dynamic = K.layers.TimeDistributed(p0, name='dynamic/tags')(rnn1)\n",
    "\n",
    "    p_static = milsed.layers.SoftMaxPool(alpha=alpha,\n",
    "                                         axis=1,\n",
    "                                         name='static/tags')(p_dynamic)\n",
    "\n",
    "    model = K.models.Model([x_mag],\n",
    "                           [p_dynamic, p_static])\n",
    "\n",
    "    model_outputs = ['dynamic/tags', 'static/tags']\n",
    "\n",
    "    return model, model_inputs, model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(working, alpha, max_samples, duration, rate,\n",
    "          batch_size, epochs, epoch_size, validation_size,\n",
    "          early_stopping, reduce_lr, seed, version):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    working : str\n",
    "        directory that contains the experiment data (h5)\n",
    "\n",
    "    alpha : float > 0\n",
    "        Alpha parameter for softmax\n",
    "\n",
    "    max_samples : int\n",
    "        Maximum number of samples per streamer\n",
    "\n",
    "    duration : float\n",
    "        Duration of training patches\n",
    "\n",
    "    batch_size : int\n",
    "        Size of batches\n",
    "\n",
    "    rate : int\n",
    "        Poisson rate for pescador\n",
    "\n",
    "    epochs : int\n",
    "        Maximum number of epoch\n",
    "\n",
    "    epoch_size : int\n",
    "        Number of batches per epoch\n",
    "\n",
    "    validation_size : int\n",
    "        Number of validation batches\n",
    "\n",
    "    early_stopping : int\n",
    "        Number of epochs before early stopping\n",
    "\n",
    "    reduce_lr : int\n",
    "        Number of epochs before reducing learning rate\n",
    "\n",
    "    seed : int\n",
    "        Random seed\n",
    "\n",
    "    version: str\n",
    "        Identifier for current model version (model ID)\n",
    "    '''\n",
    "\n",
    "    print('Load pump..')\n",
    "    # Load the pump\n",
    "    with open(os.path.join(OUTPUT_PATH, 'pump.pkl'), 'rb') as fd:\n",
    "        pump = pickle.load(fd)\n",
    "\n",
    "    # Build the sampler\n",
    "    sampler = make_sampler(max_samples, duration, pump, seed)\n",
    "\n",
    "    print('Build model..')\n",
    "    # Build the model\n",
    "    model, inputs, outputs = construct_model(pump, alpha)\n",
    "\n",
    "    print('Load index..')\n",
    "    # Load the training data\n",
    "    idx_train_ = pd.read_json(os.path.join(OUTPUT_PATH, 'index_train.json'))\n",
    "\n",
    "    print('Create splits..')\n",
    "    # Split the training data into train and validation\n",
    "    splitter_tv = ShuffleSplit(n_splits=1, test_size=0.25,\n",
    "                               random_state=seed)\n",
    "    train, val = next(splitter_tv.split(idx_train_))\n",
    "\n",
    "    idx_train = idx_train_.iloc[train]\n",
    "    idx_val = idx_train_.iloc[val]\n",
    "\n",
    "    print('Create train generator..')\n",
    "    gen_train = data_generator(working,\n",
    "                               idx_train['id'].values, sampler, epoch_size,\n",
    "                               augment=False,\n",
    "                               lam=rate,\n",
    "                               batch_size=batch_size,\n",
    "                               revive=True,\n",
    "                               random_state=seed)\n",
    "\n",
    "    print('Keras tuples..')\n",
    "    gen_train = keras_tuples(gen_train(), inputs=inputs, outputs=['static/tags'])\n",
    "\n",
    "    print('Create validation generator..')\n",
    "    gen_val = data_generator(working,\n",
    "                             idx_val['id'].values, sampler, len(idx_val),\n",
    "                             augment=False,\n",
    "                             batch_size=batch_size,\n",
    "                             revive=True,\n",
    "                             random_state=seed)\n",
    "    print('Keras tuples..')\n",
    "    gen_val = keras_tuples(gen_val(), inputs=inputs, outputs=['static/tags'])\n",
    "\n",
    "    print('Compile model..')\n",
    "    loss = {'static/tags': 'binary_crossentropy'}\n",
    "    metrics = {'static/tags': 'accuracy'}\n",
    "    monitor = 'val_loss'\n",
    "\n",
    "    model.compile(K.optimizers.Adam(), loss=loss, metrics=metrics)\n",
    "\n",
    "    # Store the model\n",
    "    model_spec = K.utils.serialize_keras_object(model)\n",
    "    with open(os.path.join(OUTPUT_PATH, version, 'model_spec.pkl'), 'wb') as fd:\n",
    "        pickle.dump(model_spec, fd)\n",
    "\n",
    "    # Construct the weight path\n",
    "    weight_path = os.path.join(OUTPUT_PATH, version, 'model.h5')\n",
    "\n",
    "    # Build the callbacks\n",
    "    cb = []\n",
    "    cb.append(K.callbacks.ModelCheckpoint(weight_path,\n",
    "                                          save_best_only=True,\n",
    "                                          verbose=1,\n",
    "                                          monitor=monitor))\n",
    "\n",
    "    cb.append(K.callbacks.ReduceLROnPlateau(patience=reduce_lr,\n",
    "                                            verbose=1,\n",
    "                                            monitor=monitor))\n",
    "\n",
    "    cb.append(K.callbacks.EarlyStopping(patience=early_stopping,\n",
    "                                        verbose=1,\n",
    "                                        monitor=monitor))\n",
    "\n",
    "    print('Fit..')\n",
    "    # Fit the model\n",
    "    model.fit_generator(gen_train, epoch_size, epochs,\n",
    "                        validation_data=gen_val,\n",
    "                        validation_steps=validation_size,\n",
    "                        callbacks=cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working = '/beegfs/js7561/datasets/dcase2017/task4_official/train/features_silence/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model version: 4e29016.7\n"
     ]
    }
   ],
   "source": [
    "version = milsed.utils.increment_version(os.path.join(OUTPUT_PATH,\n",
    "                                                          'version.txt'))\n",
    "print('Model version: {}'.format(version))\n",
    "smkdirs(os.path.join(OUTPUT_PATH, version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pump..\n",
      "Build model..\n",
      "Load index..\n",
      "Create splits..\n",
      "Create train generator..\n",
      "Keras tuples..\n",
      "Create validation generator..\n",
      "Keras tuples..\n",
      "Compile model..\n",
      "Fit..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js7561/miniconda3/envs/py35milsed/lib/python3.5/site-packages/ipykernel_launcher.py:99: UserWarning: Output \"dynamic/tags\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dynamic/tags\" during training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.1679 - static/tags_loss: 0.1679 - static/tags_acc: 0.9404"
     ]
    }
   ],
   "source": [
    "train(working, 1.0, 128, 10.0, 8, 32, 100, 512, 1024, 20, 10, 20170612, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'working': '/beegfs/js7561/datasets/dcase2017/task4_official/train/features_silence/',\n",
    "          'alpha':1.0, \n",
    "          'max_samples': 128, \n",
    "          'duration': 10.0, \n",
    "          'rate': 8,\n",
    "          'batch_size': 32, \n",
    "          'epochs': 100, \n",
    "          'epoch_size': 512, \n",
    "          'validation_size': 1024,\n",
    "          'early_stopping': 20, \n",
    "          'reduce_lr': 10, \n",
    "          'seed': 20170612, \n",
    "          'version': version}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "working = '/beegfs/js7561/datasets/dcase2017/task4_official/train/features_silence/'\n",
    "alpha =1.0\n",
    "max_samples = 128\n",
    "duration = 10.0\n",
    "rate = 8\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "epoch_size = 512\n",
    "validation_size = 1024\n",
    "early_stopping = 20\n",
    "reduce_lr = 10\n",
    "seed = 20170612\n",
    "version = version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pump..\n",
      "Build model..\n",
      "Load index..\n",
      "Create splits..\n",
      "Create train generator..\n",
      "Keras tuples..\n",
      "Create validation generator..\n",
      "Keras tuples..\n"
     ]
    }
   ],
   "source": [
    "print('Load pump..')\n",
    "# Load the pump\n",
    "with open(os.path.join(OUTPUT_PATH, 'pump.pkl'), 'rb') as fd:\n",
    "    pump = pickle.load(fd)\n",
    "\n",
    "# Build the sampler\n",
    "sampler = make_sampler(max_samples, duration, pump, seed)\n",
    "\n",
    "print('Build model..')\n",
    "# Build the model\n",
    "model, inputs, outputs = construct_model(pump, alpha)\n",
    "\n",
    "print('Load index..')\n",
    "# Load the training data\n",
    "idx_train_ = pd.read_json(os.path.join(OUTPUT_PATH, 'index_train.json'))\n",
    "\n",
    "print('Create splits..')\n",
    "# Split the training data into train and validation\n",
    "splitter_tv = ShuffleSplit(n_splits=1, test_size=0.25,\n",
    "                           random_state=seed)\n",
    "train, val = next(splitter_tv.split(idx_train_))\n",
    "\n",
    "idx_train = idx_train_.iloc[train]\n",
    "idx_val = idx_train_.iloc[val]\n",
    "\n",
    "print('Create train generator..')\n",
    "gen_train = data_generator(working,\n",
    "                           idx_train['id'].values, sampler, epoch_size,\n",
    "                           augment=False,\n",
    "                           lam=rate,\n",
    "                           batch_size=batch_size,\n",
    "                           revive=True,\n",
    "                           random_state=seed)\n",
    "\n",
    "print('Keras tuples..')\n",
    "gen_train = keras_tuples(gen_train(), inputs=inputs, outputs=outputs)\n",
    "\n",
    "print('Create validation generator..')\n",
    "gen_val = data_generator(working,\n",
    "                         idx_val['id'].values, sampler, len(idx_val),\n",
    "                         augment=False,\n",
    "                         batch_size=batch_size,\n",
    "                         revive=True,\n",
    "                         random_state=seed)\n",
    "print('Keras tuples..')\n",
    "gen_val = keras_tuples(gen_val(), inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dynamic/tags', 'static/tags']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js7561/miniconda3/envs/py35milsed/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Output \"dynamic/tags\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dynamic/tags\" during training.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print('Compile model..')\n",
    "loss = {'static/tags': 'binary_crossentropy'}\n",
    "metrics = {'static/tags': 'accuracy'}\n",
    "monitor = 'val_loss'\n",
    "\n",
    "model.compile(K.optimizers.Adam(), loss=loss, metrics=metrics)\n",
    "\n",
    "# Store the model\n",
    "model_spec = K.utils.serialize_keras_object(model)\n",
    "with open(os.path.join(OUTPUT_PATH, version, 'model_spec.pkl'), 'wb') as fd:\n",
    "    pickle.dump(model_spec, fd)\n",
    "\n",
    "# Construct the weight path\n",
    "weight_path = os.path.join(OUTPUT_PATH, version, 'model.h5')\n",
    "\n",
    "# Build the callbacks\n",
    "cb = []\n",
    "cb.append(K.callbacks.ModelCheckpoint(weight_path,\n",
    "                                      save_best_only=True,\n",
    "                                      verbose=1,\n",
    "                                      monitor=monitor))\n",
    "\n",
    "cb.append(K.callbacks.ReduceLROnPlateau(patience=reduce_lr,\n",
    "                                        verbose=1,\n",
    "                                        monitor=monitor))\n",
    "\n",
    "cb.append(K.callbacks.EarlyStopping(patience=early_stopping,\n",
    "                                    verbose=1,\n",
    "                                    monitor=monitor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dynamic/tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-9d84271ed464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-07e052b56e92>\u001b[0m in \u001b[0;36mkeras_tuples\u001b[0;34m(gen, inputs, outputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 yield ([datum[i] for i in inputs],\n\u001b[0;32m---> 60\u001b[0;31m                        [datum[o] for o in outputs])\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-07e052b56e92>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 yield ([datum[i] for i in inputs],\n\u001b[0;32m---> 60\u001b[0;31m                        [datum[o] for o in outputs])\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dynamic/tags'"
     ]
    }
   ],
   "source": [
    "for data in gen_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dynamic/tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-1f0e014f0734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-1f0e014f0734>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-07e052b56e92>\u001b[0m in \u001b[0;36mkeras_tuples\u001b[0;34m(gen, inputs, outputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 yield ([datum[i] for i in inputs],\n\u001b[0;32m---> 60\u001b[0;31m                        [datum[o] for o in outputs])\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-07e052b56e92>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 yield ([datum[i] for i in inputs],\n\u001b[0;32m---> 60\u001b[0;31m                        [datum[o] for o in outputs])\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dynamic/tags'"
     ]
    }
   ],
   "source": [
    "data = [x for x in gen_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-6137cde4893c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([430])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.time_to_frames(10.0, sr=44100, hop_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/js7561/dev/milsed/models'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = milsed.utils.load_h5('/beegfs/js7561/datasets/dcase2017/task4_official/test/features_silence/Y---lTs1dxhU_30.000_40.000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mel/mag': array([[[[-22.96606445],\n",
       "          [-20.01185226],\n",
       "          [-22.13051414],\n",
       "          ..., \n",
       "          [-79.51634216],\n",
       "          [-79.71124268],\n",
       "          [-79.71455383]],\n",
       " \n",
       "         [[-17.16335297],\n",
       "          [-21.88430023],\n",
       "          [-23.585989  ],\n",
       "          ..., \n",
       "          [-80.        ],\n",
       "          [-80.        ],\n",
       "          [-80.        ]],\n",
       " \n",
       "         [[-22.02036667],\n",
       "          [-26.22042274],\n",
       "          [-28.48296738],\n",
       "          ..., \n",
       "          [-80.        ],\n",
       "          [-80.        ],\n",
       "          [-80.        ]],\n",
       " \n",
       "         ..., \n",
       "         [[-34.1590538 ],\n",
       "          [-35.00361633],\n",
       "          [-31.60760498],\n",
       "          ..., \n",
       "          [-80.        ],\n",
       "          [-80.        ],\n",
       "          [-80.        ]],\n",
       " \n",
       "         [[-34.37856293],\n",
       "          [-36.13519669],\n",
       "          [-29.77634811],\n",
       "          ..., \n",
       "          [-80.        ],\n",
       "          [-80.        ],\n",
       "          [-80.        ]],\n",
       " \n",
       "         [[-37.751194  ],\n",
       "          [-44.70385361],\n",
       "          [-41.91004944],\n",
       "          ..., \n",
       "          [-80.        ],\n",
       "          [-80.        ],\n",
       "          [-80.        ]]]], dtype=float32),\n",
       " 'static/_valid': array([[ 0, 10]]),\n",
       " 'static/tags': array([[False, False, False, False,  True, False,  True, False, False,\n",
       "         False, False, False, False, False, False, False, False]], dtype=bool)}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
